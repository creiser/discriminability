{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "torch.manual_seed(42)\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "from twosample import get_initial_clustering, get_embeddings, merge, utils\n",
    "from twosample.affinity import average_linkage_affinity, twosample_affinity, fast_twosample_affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hp = {}\n",
    "hp['dataset'] = 'imagenet-10'\n",
    "hp['initial_clustering'] = {}\n",
    "hp['initial_clustering']['num_initial_clusters'] = 20\n",
    "hp['initial_clustering']['initial_cluster_size'] = 100\n",
    "hp['affinity'] = {}\n",
    "hp['affinity']['type'] = 'averagelinkage'\n",
    "hp['affinity']['epochs'] = 50\n",
    "hp['affinity']['batch_size'] = 64\n",
    "hp['affinity']['train_ratio'] = 0.5\n",
    "hp['affinity']['log_interval'] = -1 # disabled\n",
    "hp['device'] = torch.device('cuda')\n",
    "hp['desired_num_merges'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(hp):\n",
    "    experiment_directory = 'results/3-experiment/{}'.format(hp['dataset'])\n",
    "    utils.create_directory_if_not_exists(experiment_directory)\n",
    "    \n",
    "    experiment_filename = experiment_directory + '/{}x{}_{}.json'.format(\n",
    "        hp['initial_clustering']['num_initial_clusters'],\n",
    "        hp['initial_clustering']['initial_cluster_size'],\n",
    "        hp['affinity']['type'])\n",
    "\n",
    "    if os.path.isfile(experiment_filename):\n",
    "        print('Experiment already exists at:', experiment_filename)\n",
    "    else:\n",
    "        print('Running experiment with hyperparameters:\\n{}'.format(hp))\n",
    "        result = {}\n",
    "        transform_chain = [transforms.Resize(256),\n",
    "                           transforms.CenterCrop(224),\n",
    "                           transforms.ToTensor()]\n",
    "        dataset = datasets.ImageFolder('./datasets/' + hp['dataset'],\n",
    "                                       transform=transforms.Compose(transform_chain))\n",
    "\n",
    "        current_clustering = get_initial_clustering(hp)\n",
    "        result['initial_clustering'] = current_clustering\n",
    "\n",
    "        if hp['affinity']['type'] == 'averagelinkage':\n",
    "            embeddings = get_embeddings(hp)\n",
    "\n",
    "        result['merges'] = []\n",
    "        for merge_index in range(hp['desired_num_merges']):\n",
    "            print('Merge index:', merge_index)\n",
    "            if hp['affinity']['type'] == 'averagelinkage':\n",
    "                affinities = average_linkage_affinity(current_clustering, embeddings, hp)\n",
    "            elif hp['affinity']['type'] == 'twosample':\n",
    "                affinities = twosample_affinity(current_clustering, dataset, hp)\n",
    "            elif hp['affinity']['type'] == 'fast_twosample':\n",
    "                affinities = fast_twosample_affinity(current_clustering, dataset, hp)\n",
    "            current_clustering, cluster_a_index, cluster_b_index = merge(current_clustering, affinities)\n",
    "            result['merges'].append({'current_clustering': current_clustering,\n",
    "                                     'cluster_a_index': cluster_a_index,\n",
    "                                     'cluster_b_index': cluster_b_index,\n",
    "                                     'affinities': affinities.tolist()})\n",
    "\n",
    "        print('Writing experiment results to', experiment_filename)\n",
    "        experiment = {'hp': hp.copy(), 'result': result}\n",
    "        del experiment['hp']['device'] # cannot be serialized\n",
    "        with open(experiment_filename, 'w') as experiment_file:\n",
    "            json.dump(experiment, experiment_file, indent=4)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with hyperparameters:\n",
      "{'dataset': 'imagenet-0', 'initial_clustering': {'num_initial_clusters': 20, 'initial_cluster_size': 100}, 'affinity': {'type': 'averagelinkage', 'epochs': 50, 'batch_size': 64, 'train_ratio': 0.5, 'log_interval': -1}, 'device': device(type='cuda'), 'desired_num_merges': 7}\n",
      "Generate embeddings for imagenet-0\n",
      "Architecture: vgg16\n",
      "=> loading checkpoint 'deepcluster/pretrained/vgg16/checkpoint.pth.tar'\n",
      "=> loaded checkpoint 'deepcluster/pretrained/vgg16/checkpoint.pth.tar' (epoch 425)\n",
      "Load dataset: 0.10 s\n",
      "Compute features\n",
      "0 / 204\tTime: 3.168 (3.168)\n",
      "5 / 204\tTime: 0.171 (0.675)\n",
      "10 / 204\tTime: 0.319 (0.461)\n",
      "15 / 204\tTime: 0.173 (0.384)\n",
      "20 / 204\tTime: 0.174 (0.359)\n",
      "25 / 204\tTime: 0.174 (0.325)\n",
      "30 / 204\tTime: 0.324 (0.308)\n",
      "35 / 204\tTime: 0.175 (0.292)\n",
      "40 / 204\tTime: 0.174 (0.278)\n",
      "45 / 204\tTime: 0.172 (0.270)\n",
      "50 / 204\tTime: 0.262 (0.273)\n",
      "55 / 204\tTime: 0.349 (0.268)\n",
      "60 / 204\tTime: 0.175 (0.262)\n",
      "65 / 204\tTime: 0.174 (0.260)\n",
      "70 / 204\tTime: 0.214 (0.260)\n",
      "75 / 204\tTime: 0.221 (0.259)\n",
      "80 / 204\tTime: 0.174 (0.262)\n",
      "85 / 204\tTime: 0.175 (0.257)\n",
      "90 / 204\tTime: 0.572 (0.263)\n",
      "95 / 204\tTime: 0.173 (0.263)\n",
      "100 / 204\tTime: 0.172 (0.264)\n",
      "105 / 204\tTime: 0.527 (0.266)\n",
      "110 / 204\tTime: 0.173 (0.269)\n",
      "115 / 204\tTime: 0.174 (0.271)\n",
      "120 / 204\tTime: 0.174 (0.273)\n",
      "125 / 204\tTime: 1.153 (0.282)\n",
      "130 / 204\tTime: 0.180 (0.283)\n",
      "135 / 204\tTime: 0.173 (0.284)\n",
      "140 / 204\tTime: 0.183 (0.286)\n",
      "145 / 204\tTime: 1.220 (0.295)\n",
      "150 / 204\tTime: 0.176 (0.296)\n",
      "155 / 204\tTime: 0.183 (0.296)\n",
      "160 / 204\tTime: 0.175 (0.296)\n",
      "165 / 204\tTime: 0.821 (0.300)\n",
      "170 / 204\tTime: 0.188 (0.301)\n",
      "175 / 204\tTime: 0.187 (0.302)\n",
      "180 / 204\tTime: 0.643 (0.307)\n",
      "185 / 204\tTime: 0.677 (0.311)\n",
      "190 / 204\tTime: 0.190 (0.312)\n",
      "195 / 204\tTime: 1.194 (0.319)\n",
      "200 / 204\tTime: 0.179 (0.319)\n",
      "Write DeepCluster features to results/1-embedding/imagenet-0.npy\n",
      "Loading embeddings from results/1-embedding/imagenet-0.npy\n",
      "Writing initial clustering to results/2-initialclustering/imagenet-0/20x100.npy\n",
      "Loading embeddings from results/1-embedding/imagenet-0.npy\n",
      "Merge index: 0\n",
      "Merge index: 1\n",
      "Merge index: 2\n",
      "Merge index: 3\n",
      "Merge index: 4\n",
      "Merge index: 5\n",
      "Merge index: 6\n",
      "Writing experiment results to results/3-experiment/imagenet-0/20x100_averagelinkage.json\n",
      "Running experiment with hyperparameters:\n",
      "{'dataset': 'imagenet-0', 'initial_clustering': {'num_initial_clusters': 20, 'initial_cluster_size': 100}, 'affinity': {'type': 'twosample', 'epochs': 50, 'batch_size': 64, 'train_ratio': 0.5, 'log_interval': -1}, 'device': device(type='cuda'), 'desired_num_merges': 7}\n",
      "Loading initial clustering from results/2-initialclustering/imagenet-0/20x100.npy\n",
      "Merge index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda2/envs/pytorch1.0/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/chris/miniconda2/envs/pytorch1.0/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 7.46, mean arithmetic mean: 0.56\n",
      "Epoch: 2, Train loss: 4.47, mean arithmetic mean: 0.70\n",
      "Epoch: 3, Train loss: 3.74, mean arithmetic mean: 0.81\n",
      "Epoch: 4, Train loss: 3.37, mean arithmetic mean: 0.82\n",
      "Epoch: 5, Train loss: 3.24, mean arithmetic mean: 0.75\n",
      "Epoch: 6, Train loss: 3.32, mean arithmetic mean: 0.83\n",
      "Epoch: 7, Train loss: 2.79, mean arithmetic mean: 0.82\n",
      "Epoch: 8, Train loss: 2.75, mean arithmetic mean: 0.79\n",
      "Epoch: 9, Train loss: 2.43, mean arithmetic mean: 0.85\n",
      "Epoch: 10, Train loss: 2.26, mean arithmetic mean: 0.82\n",
      "Epoch: 11, Train loss: 2.24, mean arithmetic mean: 0.84\n"
     ]
    }
   ],
   "source": [
    "num_datasets = 10\n",
    "for dataset_index in range(num_datasets):\n",
    "    hp['dataset'] = 'imagenet-{}'.format(dataset_index)\n",
    "    for affinity_type in ['averagelinkage', 'twosample', 'fast_twosample']:\n",
    "        hp['affinity']['type'] = affinity_type\n",
    "        run_experiment(hp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.0)",
   "language": "python",
   "name": "pytorch1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
